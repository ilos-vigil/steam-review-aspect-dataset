{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%set_env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:09:47.915549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 20:09:49.075279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, AutoTokenizer,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "MAX_LENGTH = 8192\n",
    "LABELS = [\n",
    "    'label_recommended', 'label_story', 'label_gameplay', 'label_visual',\n",
    "    'label_audio', 'label_technical', 'label_price', 'label_suggestion'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x712da29eca50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples, tokenizer):\n",
    "    outputs = tokenizer(examples['cleaned_review'], truncation=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def evaluate(y_test, y_pred, labels):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Overall accuracy: {accuracy}')\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_accuracy = accuracy_score(y_test[:, idx], y_pred[:, idx])\n",
    "        print(f'Accuracy {label}: {label_accuracy}')\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f'F1 macro: {f1}')\n",
    "    print(\n",
    "        classification_report(y_test, y_pred, target_names=labels, digits=4, zero_division=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "sigmoid_v = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    model_name, gradient_accumulation_steps, num_train_epochs,\n",
    "    learning_rate, weight_decay, warmup_ratio\n",
    "):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, trust_remote_code=True,\n",
    "        num_labels=8, problem_type='multi_label_classification'\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ds_all = load_dataset('ilos-vigil/steam-review-aspect-dataset')\n",
    "    ds_all = ds_all.map(encode, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'final_{model_name.split(\"/\")[-1]}',\n",
    "        eval_strategy='no',\n",
    "        bf16=True,\n",
    "        dataloader_drop_last=False,\n",
    "        report_to='tensorboard',\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_checkpointing=True,\n",
    "        # param from ray tune\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        eval_accumulation_steps=gradient_accumulation_steps,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=ds_all['train']\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    y_pred = trainer.predict(ds_all['test'])\n",
    "    y_pred = np.where(\n",
    "        sigmoid_v(y_pred.predictions) > 0.5, 1, 0\n",
    "    ).astype(np.int32)\n",
    "\n",
    "    evaluate(np.array(ds_all['test']['labels']), y_pred, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of NewForSequenceClassification were not initialized from the model checkpoint at Alibaba-NLP/gte-large-en-v1.5 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61c44ad8d834460b0435869e8439700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1009.686, 'train_samples_per_second': 4.457, 'train_steps_per_second': 0.277, 'train_loss': 0.23595480237688338, 'epoch': 4.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e8de3421754b3ba13ac5d62056a3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.475\n",
      "Accuracy label_recommended: 0.94\n",
      "Accuracy label_story: 0.895\n",
      "Accuracy label_gameplay: 0.895\n",
      "Accuracy label_visual: 0.91\n",
      "Accuracy label_audio: 0.97\n",
      "Accuracy label_technical: 0.875\n",
      "Accuracy label_price: 0.895\n",
      "Accuracy label_suggestion: 0.905\n",
      "F1 macro: 0.8231023676252327\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "label_recommended     0.9474    0.9730    0.9600       148\n",
      "      label_story     0.8864    0.8764    0.8814        89\n",
      "   label_gameplay     0.9182    0.9481    0.9329       154\n",
      "     label_visual     0.8710    0.9310    0.9000        87\n",
      "      label_audio     0.9245    0.9608    0.9423        51\n",
      "  label_technical     0.7963    0.7544    0.7748        57\n",
      "      label_price     0.7955    0.7447    0.7692        47\n",
      " label_suggestion     0.5833    0.3333    0.4242        21\n",
      "\n",
      "        micro avg     0.8901    0.8914    0.8908       654\n",
      "        macro avg     0.8403    0.8152    0.8231       654\n",
      "     weighted avg     0.8845    0.8914    0.8865       654\n",
      "      samples avg     0.8682    0.8542    0.8454       654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best hyperparameter from 16 trials, before it stopped halfway\n",
    "# ╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
    "# │ Trial name            status         ...ccumulation_steps     num_train_epochs     learning_rate     weight_decay     warmup_ratio     iter     total time (s)     eval_loss     eval_precision     eval_recall     eval_f1 │\n",
    "# ├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
    "# │ _objective_c2d7bce8   TERMINATED                       16                    5       3.03063e-05      0.00312782         0.0196097        5            863.847      0.223566           0.913893        0.912368    0.913099 │\n",
    "# ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
    "run(\n",
    "    model_name='Alibaba-NLP/gte-large-en-v1.5',\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3.03063e-05,\n",
    "    weight_decay=0.00312782,\n",
    "    warmup_ratio=0.0196097\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
