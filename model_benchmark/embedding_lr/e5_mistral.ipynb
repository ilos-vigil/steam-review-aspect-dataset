{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes,\n",
    "\n",
    "* [Hugging Face model card](https://huggingface.co/intfloat/e5-mistral-7b-instruct) recommends last token pooling and then normalize the embedding. But average pooling without embedding normalization chosen following [model's source code](https://github.com/microsoft/unilm/tree/master/e5) for running MTEB.\n",
    "* NF4 and double quantization were used following explanation and result on paper [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# disable persistant warning shown by tokenizers\n",
    "%set_env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MODEL_NAME = 'intfloat/e5-mistral-7b-instruct'\n",
    "BATCH_SIZE = 8\n",
    "INSTRUCTION = 'Classify the aspect mentioned in the given Steam Review into up to of the eight aspects: recommended, story, gameplay, visual, audio, technical, price, and suggestion.'  # This mimic paper's string instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../dataset/v1/train.csv')\n",
    "df_test = pd.read_csv('../../dataset/v1/test.csv')\n",
    "\n",
    "labels = df_train.columns[3:].to_list()\n",
    "y_train = df_train[labels].to_numpy()\n",
    "y_test = df_test[labels].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87a473503374e2d92f354ef88d32ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralModel(\n",
       "  (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x MistralDecoderLayer(\n",
       "      (self_attn): MistralSdpaAttention(\n",
       "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): MistralMLP(\n",
       "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MistralRMSNorm()\n",
       "      (post_attention_layernorm): MistralRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): MistralRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "max_length = 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# # off-load to CPU without quantization\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     device_map='auto',\n",
    "#     offload_folder='offload',\n",
    "#     torch_dtype=torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_train, y_train, X_test, y_test, labels):\n",
    "    clf = LogisticRegression(\n",
    "        random_state=SEED,\n",
    "        max_iter=100\n",
    "    )\n",
    "    ovr = OneVsRestClassifier(clf, n_jobs=-1)\n",
    "\n",
    "    ovr.fit(X_train, y_train)\n",
    "    y_pred = ovr.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Overall accuracy: {accuracy}')\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_accuracy = accuracy_score(y_test[:, idx], y_pred[:, idx])\n",
    "        print(f'Accuracy {label}: {label_accuracy}')\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f'F1 macro: {f1}')\n",
    "    print(\n",
    "        classification_report(y_test, y_pred, target_names=labels, digits=4, zero_division=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def last_token_pool(\n",
    "        last_hidden_states: Tensor,\n",
    "        attention_mask: Tensor\n",
    "    ) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device),\n",
    "            sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def avg_pool(\n",
    "        last_hidden_states: Tensor,\n",
    "        attention_mask: Tensor,\n",
    "    ) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    # handle few torch.inf and -torch.inf occurance\n",
    "    mask_inf = emb == torch.inf\n",
    "    mask_min_inf = emb == -torch.inf\n",
    "    emb[mask_inf] = 0.0\n",
    "    emb[mask_min_inf] = 0.0\n",
    "\n",
    "    return emb\n",
    "\n",
    "def get_text_embedding(df):\n",
    "    X = np.zeros(shape=(df.shape[0], 4096), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(0, df.shape[0], BATCH_SIZE)):\n",
    "        batch_dict = tokenizer(\n",
    "            [s + INSTRUCTION for s in df.iloc[i:i+BATCH_SIZE, 2]],\n",
    "            max_length=max_length, padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        with torch.no_grad() and torch.inference_mode():\n",
    "            outputs = model(**batch_dict)\n",
    "            # embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "            embeddings = avg_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "        # embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        X[i:i+BATCH_SIZE, :] = embeddings.detach().cpu().float().numpy()\n",
    "        \n",
    "        # maybe clear stuff here\n",
    "        del batch_dict, outputs, embeddings\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/113 [00:00<?, ?it/s]2024-06-04 21:53:34.665451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 21:53:36.264677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████| 113/113 [12:35<00:00,  6.69s/it]\n",
      "100%|██████████| 25/25 [02:38<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train = get_text_embedding(df_train)\n",
    "X_test = get_text_embedding(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/mnt/db4/Git/srec/steam-review-aspect-dataset-github/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.295\n",
      "Accuracy label_recommended: 0.91\n",
      "Accuracy label_story: 0.805\n",
      "Accuracy label_gameplay: 0.905\n",
      "Accuracy label_visual: 0.745\n",
      "Accuracy label_audio: 0.84\n",
      "Accuracy label_technical: 0.865\n",
      "Accuracy label_price: 0.83\n",
      "Accuracy label_suggestion: 0.875\n",
      "F1 macro: 0.7136813015004153\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "label_recommended     0.9392    0.9392    0.9392       148\n",
      "      label_story     0.7604    0.8202    0.7892        89\n",
      "   label_gameplay     0.9299    0.9481    0.9389       154\n",
      "     label_visual     0.6957    0.7356    0.7151        87\n",
      "      label_audio     0.7317    0.5882    0.6522        51\n",
      "  label_technical     0.7586    0.7719    0.7652        57\n",
      "      label_price     0.6857    0.5106    0.5854        47\n",
      " label_suggestion     0.3750    0.2857    0.3243        21\n",
      "\n",
      "        micro avg     0.8180    0.8043    0.8111       654\n",
      "        macro avg     0.7345    0.7000    0.7137       654\n",
      "     weighted avg     0.8120    0.8043    0.8062       654\n",
      "      samples avg     0.8111    0.7914    0.7784       654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(X_train, y_train, X_test, y_test, labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
