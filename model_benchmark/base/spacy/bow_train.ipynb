{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.cli.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")\n",
    "df_test = pd.read_csv('../../../dataset/v1/test.csv')\n",
    "\n",
    "labels = df_test.columns[3:].to_list()\n",
    "y_test = df_test[labels].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output_bow\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['textcat_multilabel']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TEXTC...  CATS_SCORE  CATS_MACRO_P  CATS_MACRO_R  CATS_MACRO_F  SCORE \n",
      "---  ------  -------------  ----------  ------------  ------------  ------------  ------\n",
      "  0       0           0.25        0.00         25.00         25.00         25.00    0.25\n",
      "  0     200          44.49        0.00         25.00         25.00         25.00    0.25\n",
      "  0     400          43.18        0.00          0.00          0.00          0.00    0.00\n",
      "  0     600          39.73        0.00          0.00          0.00          0.00    0.00\n",
      "  0     800          37.18        0.00         25.00         25.00         25.00    0.25\n",
      "  1    1000          17.69        0.00         12.50         12.50         12.50    0.12\n",
      "  1    1200          16.17        0.00         12.50         12.50         12.50    0.12\n",
      "  1    1400          16.80        0.00         12.50         12.50         12.50    0.12\n",
      "  2    1600          15.36        0.00          0.00          0.00          0.00    0.00\n",
      "  2    1800           6.63        0.00          0.00          0.00          0.00    0.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output_bow/model-last\n"
     ]
    }
   ],
   "source": [
    "# ignore score since we use dummy data for dev\n",
    "train('./bow_config.cfg', output_path='./output_bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['textcat_multilabel']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('./output_bow/model-last')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_recommended': 0.48984286189079285,\n",
       " 'label_story': 0.42084258794784546,\n",
       " 'label_gameplay': 0.4618592858314514,\n",
       " 'label_visual': 0.46005651354789734,\n",
       " 'label_audio': 0.4021838307380676,\n",
       " 'label_technical': 0.4266018271446228,\n",
       " 'label_price': 0.49449148774147034,\n",
       " 'label_suggestion': 0.39189016819000244}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('I like this game so much').cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., ..., 1., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 1., ..., 1., 0., 0.],\n",
       "       [1., 0., 1., ..., 1., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.zeros(shape=y_test.shape)\n",
    "for i, doc in enumerate(nlp.pipe(df_test['cleaned_review'].to_list())):\n",
    "    y_pred[i, :] = list(doc.cats.values())\n",
    "mask_pos = y_pred > 0.5\n",
    "y_pred[mask_pos] = 1.0\n",
    "y_pred[~mask_pos] = 0.0\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.17\n",
      "Accuracy label_recommended: 0.78\n",
      "Accuracy label_story: 0.77\n",
      "Accuracy label_gameplay: 0.81\n",
      "Accuracy label_visual: 0.715\n",
      "Accuracy label_audio: 0.77\n",
      "Accuracy label_technical: 0.8\n",
      "Accuracy label_price: 0.775\n",
      "Accuracy label_suggestion: 0.89\n",
      "F1 macro: 0.5493574741399815\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "label_recommended     0.7989    0.9392    0.8634       148\n",
      "      label_story     0.7471    0.7303    0.7386        89\n",
      "   label_gameplay     0.8295    0.9481    0.8848       154\n",
      "     label_visual     0.6471    0.7586    0.6984        87\n",
      "      label_audio     0.6923    0.1765    0.2812        51\n",
      "  label_technical     0.6977    0.5263    0.6000        57\n",
      "      label_price     0.5500    0.2340    0.3284        47\n",
      " label_suggestion     0.0000    0.0000    0.0000        21\n",
      "\n",
      "        micro avg     0.7565    0.7125    0.7339       654\n",
      "        macro avg     0.6203    0.5391    0.5494       654\n",
      "     weighted avg     0.7182    0.7125    0.6950       654\n",
      "      samples avg     0.7488    0.7046    0.6997       654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred, labels):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Overall accuracy: {accuracy}')\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_accuracy = accuracy_score(y_test[:, idx], y_pred[:, idx])\n",
    "        print(f'Accuracy {label}: {label_accuracy}')\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f'F1 macro: {f1}')\n",
    "    print(\n",
    "        classification_report(y_test, y_pred, target_names=labels, digits=4, zero_division=0)\n",
    "    )\n",
    "\n",
    "evaluate(y_test, y_pred, labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
