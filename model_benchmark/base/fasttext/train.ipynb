{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../../dataset/v1/train.csv')\n",
    "df_test = pd.read_csv('../../../dataset/v1/test.csv')\n",
    "labels = df_train.columns[3:].to_list()\n",
    "labels_fasttext = [\n",
    "    x.replace('label_', '__label__')\n",
    "    for x\n",
    "    in df_train.columns[3:].to_list()\n",
    "]\n",
    "y_test = df_test[labels].to_numpy()\n",
    "\n",
    "SEED = 42\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(s: str, add_newline: bool = True):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\n', ' ', s)\n",
    "    s = re.sub(r'([a-z])[^\\sA-Za-z]([A-Za-z])', r'\\g<1>\\g<2>', s)\n",
    "    s = re.sub(r'[^\\sA-Za-z]', ' ', s)\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)\n",
    "    s = s.rstrip(' ')\n",
    "    s = s.lstrip(' ')\n",
    "    if add_newline:\n",
    "        s += '\\n'\n",
    "    return s\n",
    "\n",
    "def convert_data_final():\n",
    "    with open('./dataset/train_final.txt', 'w') as f:\n",
    "        for i in range(df_train.shape[0]):\n",
    "            label_string = ''\n",
    "            for j, type in enumerate(df_train.iloc[i, 3:].to_list()):\n",
    "                if type == 1:\n",
    "                    label_string += labels_fasttext[j] + ' '\n",
    "            if label_string == '':\n",
    "                continue\n",
    "            label_string += preprocess_review(df_train.iat[i, 2])\n",
    "            f.write(label_string)\n",
    "    \n",
    "    with open('./dataset/test_final.txt', 'w') as f:\n",
    "        for i in range(df_test.shape[0]):\n",
    "            label_string = ''\n",
    "            for j, type in enumerate(df_test.iloc[i, 3:].to_list()):\n",
    "                if type == 1:\n",
    "                    label_string += labels_fasttext[j] + ' '\n",
    "            if label_string == '':\n",
    "                continue\n",
    "            label_string += preprocess_review(df_test.iat[i, 2])\n",
    "            f.write(label_string)\n",
    "\n",
    "def convert_data_kfold(k=5):\n",
    "    X = list(range(0, df_train.shape[0]))\n",
    "    y = df_train[labels].to_numpy()\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "\n",
    "    k_idx = 0\n",
    "    for train_list, val_list in mskf.split(X, y):\n",
    "        with open(f'./dataset/train_fold_{k_idx}.txt', 'w') as f:\n",
    "            for i in train_list:\n",
    "                label_string = ''\n",
    "                for j, type in enumerate(df_train.iloc[i, 3:].to_list()):\n",
    "                    if type == 1:\n",
    "                        label_string += labels_fasttext[j] + ' '\n",
    "                    if label_string == '':\n",
    "                        continue\n",
    "                label_string += preprocess_review(df_train.iat[i, 2])\n",
    "                f.write(label_string)\n",
    "        with open(f'./dataset/val_fold_{k_idx}.txt', 'w') as f:\n",
    "            for i in val_list:\n",
    "                label_string = ''\n",
    "                for j, type in enumerate(df_train.iloc[i, 3:].to_list()):\n",
    "                    if type == 1:\n",
    "                        label_string += labels_fasttext[j] + ' '\n",
    "                    if label_string == '':\n",
    "                        continue\n",
    "                label_string += preprocess_review(df_train.iat[i, 2])\n",
    "                f.write(label_string)\n",
    "        k_idx += 1\n",
    "\n",
    "def evaluate(model, y_test, labels):\n",
    "    y_pred_label, y_pred_prob = model.predict(\n",
    "        [\n",
    "            preprocess_review(s, add_newline=False) for s in\n",
    "            df_test['cleaned_review'].to_list()\n",
    "        ],\n",
    "        k=-1\n",
    "    )\n",
    "\n",
    "    y_pred = np.zeros(shape=(df_test.shape[0], len(labels_fasttext)))\n",
    "    for row_idx, (label_list, prob_list) in enumerate(zip(y_pred_label, y_pred_prob)):\n",
    "        for label, prob in zip(label_list, prob_list):\n",
    "            col_idx = labels_fasttext.index(label)\n",
    "            y_pred[row_idx, col_idx] = prob\n",
    "\n",
    "    mask_positive = y_pred > 0.5\n",
    "    y_pred[mask_positive] = 1.0\n",
    "    y_pred[~mask_positive] = 0.0\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Overall accuracy: {accuracy}')\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_accuracy = accuracy_score(y_test[:, idx], y_pred[:, idx])\n",
    "        print(f'Accuracy {label}: {label_accuracy}')\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f'F1 macro: {f1}')\n",
    "    print(\n",
    "        classification_report(y_test, y_pred, target_names=labels, digits=4, zero_division=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run it once\n",
    "# convert_data_kfold(K)\n",
    "# convert_data_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nK=0\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nProgress: 100.0% Trials:  166 Best score:  0.421875 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13458\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread: 2041808 lr:  0.000000 avg.loss:  2.121504 ETA:   0h 0m 0s\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nautotuneDuration -> 600\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_0.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 41\\ndsub -> 8\\nepoch -> 100\\ninput -> ./dataset/train_fold_0.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.12638453969335783\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> \\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7182649704f0>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\nK=1\\nProgress: 100.0% Trials:   85 Best score:  0.417098 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13387\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread:  789435 lr:  0.000000 avg.loss:  3.257068 ETA:   0h 0m 0s\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nautotuneDuration -> 600\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_1.txt\\nbucket -> 1379641\\ncutoff -> 0\\ndim -> 100\\ndsub -> 2\\nepoch -> 100\\ninput -> ./dataset/train_fold_1.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.11763146242992435\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> \\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff78bdf0>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 2\\nws -> 5\\n==================================================\\nK=2\\nProgress: 100.0% Trials:  181 Best score:  0.415686 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13139\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread: 2276804 lr:  0.000000 avg.loss:  2.327070 ETA:   0h 0m 0s\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nautotuneDuration -> 600\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_2.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 34\\ndsub -> 4\\nepoch -> 100\\ninput -> ./dataset/train_fold_2.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.1196188275238499\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> \\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff7cde70>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\nK=3\\nProgress: 100.0% Trials:  185 Best score:  0.418848 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13175\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread: 1211246 lr:  0.000000 avg.loss:  2.286595 ETA:   0h 0m 0s\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nautotuneDuration -> 600\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_3.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 46\\ndsub -> 2\\nepoch -> 15\\ninput -> ./dataset/train_fold_3.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 1.151082828763079\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> \\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff7c7130>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\nK=4\\nProgress: 100.0% Trials:   95 Best score:  0.440154 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13074\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread:  542336 lr:  0.000000 avg.loss:  2.123288 ETA:   0h 0m 0s 0m 0s\\nautotuneDuration -> 600\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_4.txt\\nbucket -> 87038\\ncutoff -> 0\\ndim -> 84\\ndsub -> 4\\nepoch -> 86\\ninput -> ./dataset/train_fold_4.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.28655576114935943\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> \\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff7dd130>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 3\\nws -> 5\\n==================================================\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k in range(K):\n",
    "#     print(f'K={k}')\n",
    "#     model = fasttext.train_supervised(\n",
    "#         input=f'./dataset/train_fold_{k}.txt',\n",
    "#         loss='ova',\n",
    "#         autotuneValidationFile=f'./dataset/val_fold_{k}.txt',\n",
    "#         verbose=2,\n",
    "#         autotuneDuration=60*10\n",
    "#     )\n",
    "#     # get chosen training parameter\n",
    "#     model_args = model.f.getArgs()\n",
    "#     for hparam in dir(model_args):\n",
    "#         if not hparam.startswith('__'):\n",
    "#             print(f\"{hparam} -> {getattr(model_args, hparam)}\")\n",
    "#     print('='*50)\n",
    "\n",
    "# Result of K=5 fold autotune\n",
    "\n",
    "'''\n",
    "K=0\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "Progress: 100.0% Trials:  166 Best score:  0.421875 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13458\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread: 2041808 lr:  0.000000 avg.loss:  2.121504 ETA:   0h 0m 0s\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "autotuneDuration -> 600\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_0.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 41\n",
    "dsub -> 8\n",
    "epoch -> 100\n",
    "input -> ./dataset/train_fold_0.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.12638453969335783\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> \n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7182649704f0>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=1\n",
    "Progress: 100.0% Trials:   85 Best score:  0.417098 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13387\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread:  789435 lr:  0.000000 avg.loss:  3.257068 ETA:   0h 0m 0s\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "autotuneDuration -> 600\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_1.txt\n",
    "bucket -> 1379641\n",
    "cutoff -> 0\n",
    "dim -> 100\n",
    "dsub -> 2\n",
    "epoch -> 100\n",
    "input -> ./dataset/train_fold_1.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.11763146242992435\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> \n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff78bdf0>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 2\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=2\n",
    "Progress: 100.0% Trials:  181 Best score:  0.415686 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13139\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread: 2276804 lr:  0.000000 avg.loss:  2.327070 ETA:   0h 0m 0s\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "autotuneDuration -> 600\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_2.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 34\n",
    "dsub -> 4\n",
    "epoch -> 100\n",
    "input -> ./dataset/train_fold_2.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.1196188275238499\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> \n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff7cde70>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=3\n",
    "Progress: 100.0% Trials:  185 Best score:  0.418848 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13175\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread: 1211246 lr:  0.000000 avg.loss:  2.286595 ETA:   0h 0m 0s\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "autotuneDuration -> 600\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_3.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 46\n",
    "dsub -> 2\n",
    "epoch -> 15\n",
    "input -> ./dataset/train_fold_3.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 1.151082828763079\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> \n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff7c7130>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=4\n",
    "Progress: 100.0% Trials:   95 Best score:  0.440154 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13074\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread:  542336 lr:  0.000000 avg.loss:  2.123288 ETA:   0h 0m 0s 0m 0s\n",
    "autotuneDuration -> 600\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_4.txt\n",
    "bucket -> 87038\n",
    "cutoff -> 0\n",
    "dim -> 84\n",
    "dsub -> 4\n",
    "epoch -> 86\n",
    "input -> ./dataset/train_fold_4.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.28655576114935943\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> \n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7181ff7dd130>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 3\n",
    "ws -> 5\n",
    "==================================================\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.085\n",
      "Accuracy label_recommended: 0.74\n",
      "Accuracy label_story: 0.555\n",
      "Accuracy label_gameplay: 0.77\n",
      "Accuracy label_visual: 0.565\n",
      "Accuracy label_audio: 0.745\n",
      "Accuracy label_technical: 0.715\n",
      "Accuracy label_price: 0.765\n",
      "Accuracy label_suggestion: 0.895\n",
      "F1 macro: 0.2150789012273524\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "label_recommended     0.7400    1.0000    0.8506       148\n",
      "      label_story     0.0000    0.0000    0.0000        89\n",
      "   label_gameplay     0.7700    1.0000    0.8701       154\n",
      "     label_visual     0.0000    0.0000    0.0000        87\n",
      "      label_audio     0.0000    0.0000    0.0000        51\n",
      "  label_technical     0.0000    0.0000    0.0000        57\n",
      "      label_price     0.0000    0.0000    0.0000        47\n",
      " label_suggestion     0.0000    0.0000    0.0000        21\n",
      "\n",
      "        micro avg     0.7550    0.4618    0.5731       654\n",
      "        macro avg     0.1888    0.2500    0.2151       654\n",
      "     weighted avg     0.3488    0.4618    0.3974       654\n",
      "      samples avg     0.7550    0.4810    0.5657       654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14958\n",
      "Number of labels: 8\n",
      "Progress: 100.0% words/sec/thread: 1013860 lr:  0.000000 avg.loss:  4.792876 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# default param\n",
    "model = fasttext.train_supervised(\n",
    "    input=f'./dataset/train_final.txt',\n",
    "    loss='ova',\n",
    "    verbose=3,\n",
    ")\n",
    "evaluate(model, y_test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Table result of K=5 fold autotune\n",
    "\n",
    "| Param      | 0      | 1       | 2      | 3      | 4      |\n",
    "| ---------- | ------ | ------- | ------ | ------ | ------ |\n",
    "| epoch      | 100    | 100     | 100    | 15     | 86     |\n",
    "| lr         | 0.1264 | 0.1176  | 0.1196 | 1.1511 | 0.2866 |\n",
    "| dim        | 41     | 100     | 34     | 46     | 84     |\n",
    "| minCount   | 1      | 1       | 1      | 1      | 1      |\n",
    "| wordNgrams | 1      | 2       | 1      | 1      | 3      |\n",
    "| minn       | 0      | 0       | 0      | 0      | 0      |\n",
    "| maxn       | 0      | 0       | 0      | 0      | 0      |\n",
    "| bucket     | 0      | 1379641 | 0      | 0      | 87038  |\n",
    "| ws         | 5      | 5       | 5      | 5      | 5      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14958\n",
      "Number of labels: 8\n",
      "Progress: 100.0% words/sec/thread: 2025406 lr:  0.000000 avg.loss:  2.157004 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.165\n",
      "Accuracy label_recommended: 0.83\n",
      "Accuracy label_story: 0.775\n",
      "Accuracy label_gameplay: 0.815\n",
      "Accuracy label_visual: 0.7\n",
      "Accuracy label_audio: 0.785\n",
      "Accuracy label_technical: 0.785\n",
      "Accuracy label_price: 0.81\n",
      "Accuracy label_suggestion: 0.895\n",
      "F1 macro: 0.6026631868861596\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "label_recommended     0.8314    0.9662    0.8938       148\n",
      "      label_story     0.7444    0.7528    0.7486        89\n",
      "   label_gameplay     0.8545    0.9156    0.8840       154\n",
      "     label_visual     0.6452    0.6897    0.6667        87\n",
      "      label_audio     0.6000    0.4706    0.5275        51\n",
      "  label_technical     0.6750    0.4737    0.5567        57\n",
      "      label_price     0.6957    0.3404    0.4571        47\n",
      " label_suggestion     0.5000    0.0476    0.0870        21\n",
      "\n",
      "        micro avg     0.7664    0.7324    0.7490       654\n",
      "        macro avg     0.6933    0.5821    0.6027       654\n",
      "     weighted avg     0.7482    0.7324    0.7263       654\n",
      "      samples avg     0.7497    0.7124    0.7046       654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chosen param\n",
    "model = fasttext.train_supervised(\n",
    "    input=f'./dataset/train_final.txt',\n",
    "    loss='ova',\n",
    "    verbose=3,\n",
    "    epoch=100,\n",
    "    lr=0.12,\n",
    "    dim=46,\n",
    "    bucket=0,\n",
    "    ws=5,\n",
    ")\n",
    "evaluate(model, y_test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pretrained vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nK=0\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nWarning : dim is manually set to a specific value. It will not be automatically optimized.\\nProgress: 100.0% Trials:   11 Best score:  0.434896 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13458\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread:  656360 lr: -0.000100 avg.loss:  2.906421 ETA:   0h 0m 0s 656050 lr:  0.000000 avg.loss:  2.906421 ETA:   0h 0m 0s\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nWarning : dim is manually set to a specific value. It will not be automatically optimized.\\nautotuneDuration -> 1800\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_0.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 300\\ndsub -> 2\\nepoch -> 2\\ninput -> ./dataset/train_fold_0.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.13552228709214267\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> ./crawl-300d-2M.vec\\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7257715811b0>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\nK=1\\nProgress: 100.0% Trials:   11 Best score:  0.427461 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13387\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread:  654557 lr:  0.000000 avg.loss:  2.771937 ETA:   0h 0m 0s\\nautotuneDuration -> 1800\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_1.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 300\\ndsub -> 16\\nepoch -> 2\\ninput -> ./dataset/train_fold_1.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.1692979210449485\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> ./crawl-300d-2M.vec\\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x72576eb3b5b0>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\nK=2\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nWarning : dim is manually set to a specific value. It will not be automatically optimized.\\nProgress: 100.0% Trials:   11 Best score:  0.428758 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13139\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread:  701340 lr:  0.000000 avg.loss:  2.120648 ETA:   0h 0m 0s\\nautotuneDuration -> 1800\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_2.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 300\\ndsub -> 2\\nepoch -> 5\\ninput -> ./dataset/train_fold_2.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.1\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> ./crawl-300d-2M.vec\\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x72576eb5dab0>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\nK=3\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nWarning : dim is manually set to a specific value. It will not be automatically optimized.\\nProgress: 100.0% Trials:   12 Best score:  0.424084 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13175\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread:  651943 lr:  0.000000 avg.loss:  2.673541 ETA:   0h 0m 0s\\nautotuneDuration -> 1800\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_3.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 300\\ndsub -> 16\\nepoch -> 2\\ninput -> ./dataset/train_fold_3.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.1692979210449485\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> ./crawl-300d-2M.vec\\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x72576eb5ea30>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\nK=4\\nWarning : loss is manually set to a specific value. It will not be automatically optimized.\\nWarning : dim is manually set to a specific value. It will not be automatically optimized.\\nProgress: 100.0% Trials:   11 Best score:  0.455598 ETA:   0h 0m 0s\\nTraining again with best arguments\\nRead 0M words\\nNumber of words:  13074\\nNumber of labels: 8\\nProgress: 100.0% words/sec/thread:  556130 lr:  0.000000 avg.loss:  2.714420 ETA:   0h 0m 0s\\nautotuneDuration -> 1800\\nautotuneMetric -> f1\\nautotuneModelSize -> \\nautotunePredictions -> 1\\nautotuneValidationFile -> ./dataset/val_fold_4.txt\\nbucket -> 0\\ncutoff -> 0\\ndim -> 300\\ndsub -> 16\\nepoch -> 2\\ninput -> ./dataset/train_fold_4.txt\\nlabel -> __label__\\nloss -> loss_name.ova\\nlr -> 0.1692979210449485\\nlrUpdateRate -> 100\\nmaxn -> 0\\nminCount -> 1\\nminCountLabel -> 0\\nminn -> 0\\nmodel -> model_name.supervised\\nneg -> 5\\noutput -> \\npretrainedVectors -> ./crawl-300d-2M.vec\\nqnorm -> False\\nqout -> False\\nretrain -> False\\nsaveOutput -> False\\nseed -> 0\\nsetManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x725771578070>>\\nt -> 0.0001\\nthread -> 11\\nverbose -> 2\\nwordNgrams -> 1\\nws -> 5\\n==================================================\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k in range(K):\n",
    "#     print(f'K={k}')\n",
    "#     model = fasttext.train_supervised(\n",
    "#         input=f'./dataset/train_fold_{k}.txt',\n",
    "#         loss='ova',\n",
    "#         autotuneValidationFile=f'./dataset/val_fold_{k}.txt',\n",
    "#         verbose=2,\n",
    "#         autotuneDuration=60*30,\n",
    "#         dim=300,\n",
    "#         pretrainedVectors='./crawl-300d-2M.vec'\n",
    "#     )\n",
    "#     # get chosen training parameter\n",
    "#     model_args = model.f.getArgs()\n",
    "#     for hparam in dir(model_args):\n",
    "#         if not hparam.startswith('__'):\n",
    "#             print(f\"{hparam} -> {getattr(model_args, hparam)}\")\n",
    "#     print('='*50)\n",
    "\n",
    "# previous run result\n",
    "\n",
    "'''\n",
    "K=0\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
    "Progress: 100.0% Trials:   11 Best score:  0.434896 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13458\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread:  656360 lr: -0.000100 avg.loss:  2.906421 ETA:   0h 0m 0s 656050 lr:  0.000000 avg.loss:  2.906421 ETA:   0h 0m 0s\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
    "autotuneDuration -> 1800\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_0.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 300\n",
    "dsub -> 2\n",
    "epoch -> 2\n",
    "input -> ./dataset/train_fold_0.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.13552228709214267\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> ./crawl-300d-2M.vec\n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7257715811b0>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=1\n",
    "Progress: 100.0% Trials:   11 Best score:  0.427461 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13387\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread:  654557 lr:  0.000000 avg.loss:  2.771937 ETA:   0h 0m 0s\n",
    "autotuneDuration -> 1800\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_1.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 300\n",
    "dsub -> 16\n",
    "epoch -> 2\n",
    "input -> ./dataset/train_fold_1.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.1692979210449485\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> ./crawl-300d-2M.vec\n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x72576eb3b5b0>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=2\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
    "Progress: 100.0% Trials:   11 Best score:  0.428758 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13139\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread:  701340 lr:  0.000000 avg.loss:  2.120648 ETA:   0h 0m 0s\n",
    "autotuneDuration -> 1800\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_2.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 300\n",
    "dsub -> 2\n",
    "epoch -> 5\n",
    "input -> ./dataset/train_fold_2.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.1\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> ./crawl-300d-2M.vec\n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x72576eb5dab0>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=3\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
    "Progress: 100.0% Trials:   12 Best score:  0.424084 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13175\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread:  651943 lr:  0.000000 avg.loss:  2.673541 ETA:   0h 0m 0s\n",
    "autotuneDuration -> 1800\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_3.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 300\n",
    "dsub -> 16\n",
    "epoch -> 2\n",
    "input -> ./dataset/train_fold_3.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.1692979210449485\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> ./crawl-300d-2M.vec\n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x72576eb5ea30>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "K=4\n",
    "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
    "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
    "Progress: 100.0% Trials:   11 Best score:  0.455598 ETA:   0h 0m 0s\n",
    "Training again with best arguments\n",
    "Read 0M words\n",
    "Number of words:  13074\n",
    "Number of labels: 8\n",
    "Progress: 100.0% words/sec/thread:  556130 lr:  0.000000 avg.loss:  2.714420 ETA:   0h 0m 0s\n",
    "autotuneDuration -> 1800\n",
    "autotuneMetric -> f1\n",
    "autotuneModelSize -> \n",
    "autotunePredictions -> 1\n",
    "autotuneValidationFile -> ./dataset/val_fold_4.txt\n",
    "bucket -> 0\n",
    "cutoff -> 0\n",
    "dim -> 300\n",
    "dsub -> 16\n",
    "epoch -> 2\n",
    "input -> ./dataset/train_fold_4.txt\n",
    "label -> __label__\n",
    "loss -> loss_name.ova\n",
    "lr -> 0.1692979210449485\n",
    "lrUpdateRate -> 100\n",
    "maxn -> 0\n",
    "minCount -> 1\n",
    "minCountLabel -> 0\n",
    "minn -> 0\n",
    "model -> model_name.supervised\n",
    "neg -> 5\n",
    "output -> \n",
    "pretrainedVectors -> ./crawl-300d-2M.vec\n",
    "qnorm -> False\n",
    "qout -> False\n",
    "retrain -> False\n",
    "saveOutput -> False\n",
    "seed -> 0\n",
    "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x725771578070>>\n",
    "t -> 0.0001\n",
    "thread -> 11\n",
    "verbose -> 2\n",
    "wordNgrams -> 1\n",
    "ws -> 5\n",
    "==================================================\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14958\n",
      "Number of labels: 8\n",
      "Progress:  93.1% words/sec/thread:  667571 lr:  0.006859 avg.loss:  2.286446 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.17\n",
      "Accuracy label_recommended: 0.84\n",
      "Accuracy label_story: 0.79\n",
      "Accuracy label_gameplay: 0.82\n",
      "Accuracy label_visual: 0.745\n",
      "Accuracy label_audio: 0.79\n",
      "Accuracy label_technical: 0.81\n",
      "Accuracy label_price: 0.765\n",
      "Accuracy label_suggestion: 0.89\n",
      "F1 macro: 0.5871326672275503\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "label_recommended     0.8372    0.9730    0.9000       148\n",
      "      label_story     0.7582    0.7753    0.7667        89\n",
      "   label_gameplay     0.8642    0.9091    0.8861       154\n",
      "     label_visual     0.6875    0.7586    0.7213        87\n",
      "      label_audio     0.6364    0.4118    0.5000        51\n",
      "  label_technical     0.7436    0.5088    0.6042        57\n",
      "      label_price     0.5000    0.2340    0.3188        47\n",
      " label_suggestion     0.0000    0.0000    0.0000        21\n",
      "\n",
      "        micro avg     0.7792    0.7339    0.7559       654\n",
      "        macro avg     0.6284    0.5713    0.5871       654\n",
      "     weighted avg     0.7380    0.7339    0.7272       654\n",
      "      samples avg     0.7606    0.7179    0.7113       654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  627994 lr:  0.000000 avg.loss:  2.230373 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# default param\n",
    "model = fasttext.train_supervised(\n",
    "    input=f'./dataset/train_final.txt',\n",
    "    loss='ova',\n",
    "    verbose=3,\n",
    "    dim=300,\n",
    "    pretrainedVectors='./pretrained_vector/crawl-300d-2M.vec'\n",
    ")\n",
    "evaluate(model, y_test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Param      | 0      | 1       | 2      | 3      | 4      |\n",
    "| ---------- | ------ | ------- | ------ | ------ | ------ |\n",
    "| epoch      | 2      | 2       | 5      | 2      | 2      |\n",
    "| lr         | 0.1355 | 0.1693  | 0.1    | 0.1693 | 0.1693 |\n",
    "| minCount   | 1      | 1       | 1      | 1      | 1      |\n",
    "| wordNgrams | 1      | 1       | 1      | 1      | 1      |\n",
    "| minn       | 0      | 0       | 0      | 0      | 0      |\n",
    "| maxn       | 0      | 0       | 0      | 0      | 0      |\n",
    "| bucket     | 0      | 0       | 0      | 0      | 0      |\n",
    "| dsub       | 2      | 16      | 2      | 16     | 16     |\n",
    "| ws         | 5      | 5       | 5      | 5      | 5      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14958\n",
      "Number of labels: 8\n",
      "Progress: 100.0% words/sec/thread:  574372 lr:  0.000000 avg.loss:  2.801757 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.175\n",
      "Accuracy label_recommended: 0.835\n",
      "Accuracy label_story: 0.775\n",
      "Accuracy label_gameplay: 0.795\n",
      "Accuracy label_visual: 0.755\n",
      "Accuracy label_audio: 0.77\n",
      "Accuracy label_technical: 0.795\n",
      "Accuracy label_price: 0.775\n",
      "Accuracy label_suggestion: 0.895\n",
      "F1 macro: 0.5646531270349565\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "label_recommended     0.8324    0.9730    0.8972       148\n",
      "      label_story     0.7340    0.7753    0.7541        89\n",
      "   label_gameplay     0.8383    0.9091    0.8723       154\n",
      "     label_visual     0.7065    0.7471    0.7263        87\n",
      "      label_audio     0.5926    0.3137    0.4103        51\n",
      "  label_technical     0.7353    0.4386    0.5495        57\n",
      "      label_price     0.5556    0.2128    0.3077        47\n",
      " label_suggestion     0.0000    0.0000    0.0000        21\n",
      "\n",
      "        micro avg     0.7752    0.7171    0.7450       654\n",
      "        macro avg     0.6243    0.5462    0.5647       654\n",
      "     weighted avg     0.7299    0.7171    0.7097       654\n",
      "      samples avg     0.7593    0.7062    0.7058       654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chosen param\n",
    "model = fasttext.train_supervised(\n",
    "    input=f'./dataset/train_final.txt',\n",
    "    loss='ova',\n",
    "    verbose=3,\n",
    "    dim=300,\n",
    "    pretrainedVectors='./pretrained_vector/crawl-300d-2M.vec',\n",
    "    epoch=2,\n",
    "    lr=0.1693,\n",
    "    minCount=1,\n",
    "    wordNgrams=1,\n",
    "    minn=0,\n",
    "    maxn=0,\n",
    "    bucket=0,\n",
    "    ws=5,\n",
    ")\n",
    "evaluate(model, y_test, labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
